import csv
from bs4 import BeautifulSoup
import requests
import time
import pandas as pd

# ===== æ‰‹åŠ¨é…ç½®é¡¹ =====

BASE_URL = 'http://example.com'  # æ•™åŠ¡ç³»ç»ŸåŸŸåï¼Œæ­¤å¤„ä¸åŠ /eamsåç¼€
EAMS_COOKIE = 'semester.id=xxx; JSESSIONID=xxx; SERVERNAME=xxx; GSESSIONID=xxx'  # æ•™åŠ¡ç³»ç»Ÿç™»å½•åçš„ Cookie å€¼
EAMS_SEMESTER_ID = 'xxx' # ä¸‹è¿°postè¯·æ±‚ä¸­çš„semester.idå‚æ•°å€¼
EAMS_UNDERLINE = 'xxx' # ä¸‹è¿°postè¯·æ±‚ä¸­çš„_å‚æ•°å€¼

 
 
# ===== å…¶ä»–é…ç½®é¡¹ =====
TEACHER_LIST_OUTPUT_CSV = 'teacher_list.csv'
LESSONS_LIST_OUTPUT_CSV = 'lessons_list.csv'
LESSONS_DEDUP_LIST_OUTPUT_CSV = 'lessons_list_dedup.csv'
CLASSROOM_LIST_OUTPUT_TXT = 'classroom_list.txt'
LOG_FILE = 'process.log'


# ===== è‡ªå®šä¹‰ print + log å‡½æ•° =====
def log_print(*args, **kwargs):
    """åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶"""
    # æ„é€ è¦è¾“å‡ºçš„å­—ç¬¦ä¸²ï¼ˆæ¨¡æ‹Ÿ print çš„é»˜è®¤è¡Œä¸ºï¼‰
    sep = kwargs.get('sep', ' ')
    end = kwargs.get('end', '\n')
    message = sep.join(str(arg) for arg in args) + end

    # æ‰“å°åˆ°æ§åˆ¶å°
    print(message, end='')  # æ³¨æ„ï¼šmessage å·²åŒ…å« end

    # è¿½åŠ å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆä½¿ç”¨ UTF-8ï¼‰
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(message)


# ===== å¯¹æ•™åŠ¡ç³»ç»Ÿ-æ•™å¸ˆå…¬å…±è¯¾è¡¨è¿›è¡Œè¯·æ±‚ =====

session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
    'Accept': '*/*',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
    'Cookie': EAMS_COOKIE,
})

session.get(BASE_URL + '/eams/studentPublicScheduleQuery!search.action')

time.sleep(1)  # ç­‰å¾… 1 ç§’ï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¿«

session.headers.update({
    'referer': BASE_URL + '/eams/studentPublicScheduleQuery!search.action',
    'Accept': '*/*',
    'Accept-Encoding': 'gzip, deflate',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    'Cookie': EAMS_COOKIE,
    'Host': BASE_URL.replace('http://', '').replace('https://', ''),
    'Origin': BASE_URL,
    'Pragma': 'no-cache',
    'X-Requested-With': 'XMLHttpRequest'
})


html_content = session.post(
            url = BASE_URL + '/eams/studentPublicScheduleQuery!search.action',
            data = f'semester.id={EAMS_SEMESTER_ID}&courseTableType=teacher&_={EAMS_UNDERLINE}&pageNo=1&pageSize=10000'
).text



# ===== è§£æ HTML =====
soup = BeautifulSoup(html_content, 'html.parser')

teachers = []
a_tags = soup.find_all('a', href=True)

for idx, a in enumerate(a_tags, start=1):
    name = a.get_text(strip=True)
    relative_link = a['href']
    full_link = BASE_URL + relative_link

    # åˆå§‹åŒ–é»˜è®¤å€¼ä¸º "ç©º"
    gender = "ç©º"
    department = "ç©º"

    # æ‰¾åˆ°æ‰€åœ¨è¡Œï¼ˆ<tr>ï¼‰
    td_name = a.find_parent('td')
    if td_name:
        row = td_name.find_parent('tr')
        if row:
            tds = row.find_all('td')
            # å‡è®¾ç»“æ„ï¼š[0:ç©º] | [1:å§“å] | [2:æ€§åˆ«] | [3:é™¢ç³»]
            if len(tds) > 2:
                g_text = tds[2].get_text(strip=True)
                gender = g_text if g_text else "ç©º"
            if len(tds) > 3:
                d_text = tds[3].get_text(strip=True)
                department = d_text if d_text else "ç©º"
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šé€šè¿‡å…¨å±€ td åˆ—è¡¨å®šä½ï¼ˆé€‚ç”¨äºæ—  <tr> çš„æƒ…å†µï¼‰
            all_tds = soup.find_all('td')
            try:
                i = all_tds.index(td_name)
                if i + 1 < len(all_tds):
                    g_text = all_tds[i + 1].get_text(strip=True)
                    gender = g_text if g_text else "null"
                if i + 2 < len(all_tds):
                    d_text = all_tds[i + 2].get_text(strip=True)
                    department = d_text if d_text else "null"
            except ValueError:
                pass  # ä¿æŒé»˜è®¤ "null"

    teachers.append({
        'åºå·': idx,
        'å§“å': name,
        'æ€§åˆ«': gender,
        'é™¢ç³»': department,
        'é“¾æ¥': full_link
    })

# ===== å°†æ•™å¸ˆä¿¡æ¯å†™å…¥ CSV =====
with open(TEACHER_LIST_OUTPUT_CSV, 'w', encoding='utf-8-sig', newline='') as csvfile:
    fieldnames = ['åºå·', 'å§“å', 'æ€§åˆ«', 'é™¢ç³»', 'é“¾æ¥']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(teachers)

log_print(f"âœ… å·²æˆåŠŸæå– {len(teachers)} ä½æ•™å¸ˆä¿¡æ¯ï¼Œå¹¶ä¿å­˜è‡³ '{TEACHER_LIST_OUTPUT_CSV}'")

time.sleep(1)  # ç­‰å¾… 1 ç§’ï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¿«

# ===== åˆå§‹åŒ–è¯¾ç¨‹è¾“å‡º CSV =====
output_headers = [
    "åºå·", "è¯¾ç¨‹åºå·", "è¯¾ç¨‹ä»£ç ", "è¯¾ç¨‹åç§°", "è¯¾ç¨‹ç±»åˆ«", "æ•™å­¦ç­",
    "å‘¨è¯¾æ—¶", "å­¦åˆ†", "æˆè¯¾è¯­è¨€", "ä¸Šè¯¾äººæ•°", "æ˜¯å¦æ’è¯¾", "å‘¨æ¬¡",
    "æ˜ŸæœŸ", "èŠ‚æ¬¡", "æˆè¯¾æ•™å¸ˆ", "ä¸Šè¯¾åœ°ç‚¹", "å¤‡æ³¨"
]

with open(LESSONS_LIST_OUTPUT_CSV, 'w', encoding='utf-8-sig', newline='') as f:
    csv.writer(f).writerow(output_headers)

total_extracted = 0


# ===== å¼€å§‹å¤„ç†è¯¾ç¨‹åˆ—è¡¨ =====
log_print(f"å¼€å§‹å¤„ç†æ•™å¸ˆæ’è¯¾æ•°æ®ï¼Œè¾“å…¥æ–‡ä»¶: {TEACHER_LIST_OUTPUT_CSV}")

try:
    with open(TEACHER_LIST_OUTPUT_CSV, 'r', encoding='utf-8-sig') as f_in:
        reader = csv.reader(f_in)
        header = next(reader, None)  # è·³è¿‡è¡¨å¤´

        for row_idx, row in enumerate(reader, start=1):
            if len(row) < 5:
                log_print(f"[{row_idx}] âš ï¸ è¡Œæ•°æ®ä¸è¶³5åˆ—ï¼Œè·³è¿‡: {row}")
                continue

            teacher_info = row[1].strip()
            url = row[4].strip()

            if not url or not url.startswith('http'):
                log_print(f"[{row_idx}] âš ï¸ æ— æ•ˆURLï¼Œè·³è¿‡æ•™å¸ˆ: {teacher_info}")
                continue

            retry_count = 0
            while True:
                retry_count += 1
                if retry_count > 1:
                    log_print(f"[{row_idx}] æ•™å¸ˆ {teacher_info} ç¬¬ {retry_count} æ¬¡é‡è¯•...")

                try:
                    log_print(f"[{row_idx}] æ­£åœ¨è¯·æ±‚æ•™å¸ˆ: {teacher_info} | URL: {url}")

                    resp = session.get(url, timeout=15)
                    resp.raise_for_status()
                    resp.encoding = 'utf-8'
                    html = resp.text

                    soup = BeautifulSoup(html, 'html.parser')
                    scheduled_span = soup.find('span', string='å·²æ’è¯¾')

                    course_rows = []
                    if scheduled_span:
                        scheduled_table = scheduled_span.find_next('table')
                        if scheduled_table:
                            data_trs = scheduled_table.find_all('tr')[1:]
                            for tr in data_trs:
                                if tr.find('td', colspan=True):
                                    continue
                                tds = tr.find_all(['td', 'th'])
                                if not tds:
                                    continue
                                row_data = []
                                for td in tds:
                                    text = td.get_text(strip=True)
                                    row_data.append(text if text else "null")
                                while len(row_data) < 17:
                                    row_data.append("null")
                                course_rows.append(row_data[:17])

                    with open(LESSONS_LIST_OUTPUT_CSV, 'a', encoding='utf-8-sig', newline='') as out_f:
                        csv.writer(out_f).writerows(course_rows)

                    current_count = len(course_rows)
                    total_extracted += current_count
                    log_print(f"âœ… æ•™å¸ˆ {teacher_info} æˆåŠŸæå– {current_count} æ¡è¯¾ç¨‹ï¼ˆç¬¬ {retry_count} æ¬¡å°è¯•ï¼‰")
                    break

                except Exception as e:
                    log_print(f"âŒ æ•™å¸ˆ {teacher_info} å¤„ç†å¤±è´¥ï¼ˆç¬¬ {retry_count} æ¬¡ï¼‰: {e}")
                    log_print("   â†’ 1ç§’åé‡è¯•...")
                    time.sleep(1)

            time.sleep(1)

    log_print(f"\nğŸ‰ æ‰€æœ‰æ•™å¸ˆå¤„ç†å®Œæ¯•ï¼å…±æå– {total_extracted} æ¡å·²æ’è¯¾è®°å½•ï¼Œä¿å­˜è‡³ '{LESSONS_LIST_OUTPUT_CSV}'")

except Exception as e:
    log_print(f"ğŸ’¥ ä¸»ç¨‹åºå´©æºƒ: {e}")
    raise


# ===== å¯¹è¯¾ç¨‹è¿›è¡Œå»é‡ =====

# è¯»å– CSV æ–‡ä»¶
df = pd.read_csv(LESSONS_LIST_OUTPUT_CSV)

# æŒ‡å®šè¦æ‹¼æ¥çš„åˆ—ï¼ˆæŒ‰0èµ·å§‹ç´¢å¼•ï¼‰
cols_to_combine = [2, 5, 11, 12, 13, 15]  # å¯¹åº”ç¬¬3,6,12,13,14,16åˆ—

# æ£€æŸ¥åˆ—ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
max_col_index = df.shape[1] - 1
if max(cols_to_combine) > max_col_index:
    raise IndexError("æŒ‡å®šçš„åˆ—ç´¢å¼•è¶…å‡ºCSVæ–‡ä»¶å®é™…åˆ—æ•°ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚")

# æå–è¿™äº›åˆ—ï¼Œå¹¶å°†æ¯è¡Œæ‹¼æ¥ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ä½œä¸ºå”¯ä¸€æ ‡è¯†
# ä½¿ç”¨ fillna('') é˜²æ­¢ NaN å¯¼è‡´æ‹¼æ¥å‡ºé—®é¢˜
df_subset = df.iloc[:, cols_to_combine].fillna('').astype(str)
df['combined_key'] = df_subset.apply('_|_'.join, axis=1)  # ä½¿ç”¨ç‰¹æ®Šåˆ†éš”ç¬¦é¿å…å­—æ®µæ··æ·†

# å»é‡ï¼šä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è¡Œï¼ˆåŸºäº combined_keyï¼‰
df_dedup = df.drop_duplicates(subset='combined_key', keep='first')

# åˆ é™¤è¾…åŠ©åˆ—
df_dedup = df_dedup.drop(columns=['combined_key'])

# ä¿å­˜ç»“æœåˆ°æ–°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
df_dedup.to_csv('lessons_list_dedup.csv', index=False, encoding='utf-8-sig')

log_print("\nè¯¾ç¨‹å»é‡")
log_print(f"åŸå§‹è¡Œæ•°: {len(df)}")
log_print(f"å»é‡åè¡Œæ•°: {len(df_dedup)}")
log_print(f"å»é‡ç»“æœå·²ä¿å­˜åˆ° {LESSONS_DEDUP_LIST_OUTPUT_CSV}")

# ===== å»é™¤æ•™å®¤åçš„æ˜Ÿå· =====
# è¯»å– CSV æ–‡ä»¶
file_path = LESSONS_DEDUP_LIST_OUTPUT_CSV
df = pd.read_csv(file_path, dtype=str)  # ä»¥å­—ç¬¦ä¸²ç±»å‹è¯»å–ï¼Œé¿å…ç±»å‹é—®é¢˜

# æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘16åˆ—
if df.shape[1] < 16:
    raise ValueError(f"CSV æ–‡ä»¶åˆ—æ•°ä¸è¶³16åˆ—ï¼Œå½“å‰åªæœ‰ {df.shape[1]} åˆ—ã€‚")

# å»é™¤ç¬¬16åˆ—ï¼ˆç´¢å¼•15ï¼‰ä¸­çš„æ‰€æœ‰æ˜Ÿå· *
df.iloc[:, 15] = df.iloc[:, 15].astype(str).str.replace('*', '', regex=False)

# å†™å›åŸæ–‡ä»¶ï¼ˆè¦†ç›–ï¼‰
df.to_csv(file_path, index=False, encoding='utf-8-sig')

log_print(f"å·²æˆåŠŸå»é™¤ {file_path} ä¸­ç¬¬16åˆ—çš„æ‰€æœ‰ '*'ï¼Œå¹¶ä¿å­˜å›åŸæ–‡ä»¶ã€‚")


# ===== å¤„ç†æ•™å®¤åˆ—è¡¨ =====
df = pd.read_csv(LESSONS_LIST_OUTPUT_CSV)

if 'ä¸Šè¯¾åœ°ç‚¹' in df.columns:
    p_column = df['ä¸Šè¯¾åœ°ç‚¹']

# ç”¨äºå»é‡å’Œä¿æŒé¡ºåº
seen = set()
unique_items = []

# éå† P åˆ—çš„æ¯ä¸€è¡Œï¼ˆè·³è¿‡ç©ºå€¼ï¼‰
for value in p_column.dropna():
    # è½¬ä¸ºå­—ç¬¦ä¸²å¹¶å»é™¤é¦–å°¾ç©ºç™½
    str_value = str(value).strip()
    
    # æŒ‰é€—å·åˆ†å‰²ï¼ˆæ”¯æŒâ€œa,b,câ€å½¢å¼ï¼‰
    if ',' in str_value:
        parts = [part.strip() for part in str_value.split(',')]
    else:
        parts = [str_value]
    
    # å¤„ç†æ¯ä¸ªéƒ¨åˆ†
    for part in parts:
        # å»é™¤æ˜Ÿå· *
        cleaned = part.replace('*', '').strip()
        # å¿½ç•¥ç©ºå­—ç¬¦ä¸²ï¼Œå¹¶å»é‡
        if cleaned and cleaned not in seen:
            seen.add(cleaned)
            unique_items.append(cleaned)

# å†™å…¥ txt æ–‡ä»¶ï¼Œä¸€è¡Œä¸€ä¸ª
with open(CLASSROOM_LIST_OUTPUT_TXT, 'w', encoding='utf-8') as f:
    for item in unique_items:
        f.write("é»˜è®¤æ ¡åŒº:é»˜è®¤æ¥¼å®‡:" + item + '\n')

print(f"å…±å†™å…¥ {len(unique_items)} ä¸ªæ•™å®¤åˆ° {CLASSROOM_LIST_OUTPUT_TXT}")


# ===== å¯¹æ•™å®¤åˆ—è¡¨æ–‡ä»¶æŒ‰å‡åºæ’åº =====
file_path = CLASSROOM_LIST_OUTPUT_TXT

with open(file_path, 'r', encoding='utf-8') as f:
    lines = f.readlines()

# å»é™¤æ¯è¡Œæœ«å°¾çš„æ¢è¡Œç¬¦ï¼ˆä¾¿äºæ’åºï¼‰ï¼Œå¹¶è¿‡æ»¤ç©ºè¡Œï¼ˆå¯é€‰ï¼‰
stripped_lines = [line.rstrip('\n\r') for line in lines]

# å‡åºæ’åºï¼ˆé»˜è®¤æ˜¯å­—å…¸åºï¼ŒåŒºåˆ†å¤§å°å†™ï¼‰
sorted_lines = sorted(stripped_lines)

# å°†æ’åºåçš„è¡Œé‡æ–°åŠ ä¸Šæ¢è¡Œç¬¦
output_lines = [line + '\n' for line in sorted_lines]

# å†™å›åŸæ–‡ä»¶
with open(file_path, 'w', encoding='utf-8') as f:
    f.writelines(output_lines)

log_print(f"æ•™å®¤åˆ—è¡¨ {file_path} å·²æŒ‰å‡åºæ’åºå¹¶ä¿å­˜ã€‚")